
\documentclass{article}
\title{Reading Notes for Bayesian Model Selection}
\author{Xiang Pan}

\usepackage{url}
\usepackage{titling}
\usepackage{geometry}

% \geometry{a4paper,scale=0.9,left=10mm, right=10mm, top=3mm, bottom=20mm}
\geometry{a4paper,scale=0.9}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{tikz}
\def\ci{\perp\!\!\!\perp}
\usetikzlibrary{fit,positioning}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=cyan,
}


\begin{document}
\maketitle
\section{Occam’s Razor}

Occam’s Razor focus on complex models.

\textbf{Non-Bayesian} Netease Music
Model complexity is often regulated by adjusting the number of free parameters in the model and sometimes complexity is further constrained by the use of regularizers (such as weight decay).

Depending on the scaling properties of the prior over parameters, both the Occam’s Razor view and the large models view can seem appropriate.

\textbf{Bayesian}
\begin{itemize}
    \item One view is to infer the probability of the model for each of several different model sizes and use these probabilities when making predictions.
    \item we simply choose a “large enough” model and sidestep the problem of model size selection.
\end{itemize}

\textbf{View1}
The evidence is the probability that if you randomly selected parameter values from your model class, you would generate data set Y .

\textbf{View2}
We don’t seriously believe that the “true” generative process can be implemented exactly with a small model. Moreover, optimizing (or integrating) over continuous hyperparameters may be easier than optimizing over the discrete space of model sizes.

We ought not to limit the number of basis functions in function approximation a priori since we don’t really believe that the data was actually generated from a small number of ﬁxed basis functions.


scaling exponent $\gamma$: Large values of $\gamma$ correspond to priors with most probability mass on simple functions, whereas small values of $\gamma$ correspond to priors that allow more complex functions.



\section{Bayesian model averaging is not model combination}
\begin{itemize}
    \item Model combination works by enriching the space of hypotheses, not by approximating a Bayesian model average.
    \item BMA is ‘soft model selection’. The soft weights in BMA only reflect a statistical inability to distinguish the hypothesis based on limited data.
\end{itemize}

\textbf{BMA as soft model selection}

Bayesian model averaging is best thought of as a method for ‘soft model selection.’ It answers the question: “Given that all of the data so far was generated by exactly one of the hypotheses, what is the probability of observing the new pair (c, x)?”

\textbf{BMA for stacked models}

“Given that all of the data so far was generated by some linear combination of the hypotheses, what is the probability of observing the new pair (c, x)?”

This is BMA applied to a new hypotheses space of “stacked” models. 

\begin{equation}
    p((c, x) \mid D) \propto \sum_{h} p((c, x), D \mid h) p(h)
\end{equation},
which emphasizes the assumption that exactly one hypothesis is responsible for all of the data.

\section{Bayesian Averaging of Classiﬁers and the Overﬁtting Problem\cite{domingos__BayesianAveragingClassifiers}}
\textbf{Bayesian model averaging}
posterior probability is the product of the model’s prior probability, which reﬂects our domain knowledge (or assumptions) before collecting data, likelihood, which is the probability of the data given the model, \footnote{You can get the updated and full version of note at \url{https://github.com/Xiang-Pan/NYU_Bayesian_Machine_Learning/blob/master/reading_notes/reading6/build/note6.pdf}}

Given the “correct” model space and prior distribution, Bayesian model averaging is the optimal method for making predictions; 
% using only the model with highest posterior can be an acceptable approximation.

\subsection{Bayesian Model Averaging in Classiﬁcation}

\begin{equation}
    \operatorname{Pr}(h \mid \vec{x}, \vec{c})=\frac{\operatorname{Pr}(h)}{\operatorname{Pr}(\vec{x}, \vec{c})} \prod_{i=1}^{n} \operatorname{Pr}\left(x_{i}, c_{i} \mid h\right)
\end{equation}

\begin{equation}
    \text{posterior probability of h given datasets} = \frac{\text{prior probability of h} }{\text{data prior}}\times \text{likelihood}
\end{equation}

Each example’s class is corrupted with probability $\epsilon$,

\begin{equation}
    \operatorname{Pr}(h \mid \vec{x}, \vec{c}) \propto \operatorname{Pr}(h)(1-\epsilon)^{s} \epsilon^{n-s},
\end{equation}
where s is the number of examples correctly classiﬁed by h.

An unseen example x is assigned to the class that maximizes:
\begin{equation}
    \operatorname{Pr}(c \mid x, \vec{x}, \vec{c}, H)=\sum_{h \in H} \operatorname{Pr}(c \mid x, h) \operatorname{Pr}(h \mid \vec{x}, \vec{c})
\end{equation}

\subsection{Bayesian Model Averaging of Bagged C4.5 Rule Sets}

\begin{equation}
    \sum f(x) p(x)=\sum f(x)\left[\frac{p(x)}{q(x)}\right] q(x)
\end{equation}

p(x) being the model posterior probabilities, importance sampling distribution q(x).


Bagging($p(x) = q(x)$) can be viewed as an approximation of Bayesian model averaging by importance sampling.

More closely approximate: weighting models by their posteriors leads to a better approximation of Bayesian model averaging than weighting them uniformly


\subsection{The Overfitting Problem in Bayesian Model Averaging}

Bayesian model averaging in effect acts more like model selection than model averaging, and is equivalent to amplifying the search process that produces overﬁtting in the underlying learner in the first place.

Overfitting is the result of attempting too many hypotheses, and consequently ﬁnding a poor hypothesis that appears good. In this light, Bayesian model averaging’s potential to aggravate the overﬁtting problem relative to learning a single model becomes clear.

Bagging is at one extreme, with uniform weights, and never increases Overﬁtting. Methods like boosting and stacking produce weights that are more variable, and can sometimes lead to overﬁtting.Bayesian model averaging is at the opposite end of the spectrum, producing highly asymmetric weights, and being correspondingly more prone to overﬁtting.

\section{Bayesian PCA\cite{minka__AutomaticChoiceDimensionality}}

\subsection{Probabilistic PCA}
\begin{equation}
    \begin{aligned}
    \mathbf{x} &=\sum_{j=1}^{k} \mathbf{h}_{j} w_{j}+\mathbf{m}+\mathbf{e} \\
    &=\mathbf{H} \mathbf{w}+\mathbf{m}+\mathbf{e} \\
    p(\mathbf{e}) & \sim \mathcal{N}(\mathbf{0}, \mathbf{V})
    \end{aligned}
\end{equation}

k $<$ d(dimension of x).

The goal of PCA is to estimate the basis vectors H and the noise variance $v$ from a data set D.



\bibliographystyle{plain}
\bibliography{note6}
\appendix
\end{document}